{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noise Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text']\n"
     ]
    }
   ],
   "source": [
    "noise_list=['this','is','am','a','are']\n",
    "def Remove_noise(input_text):\n",
    "    words=input_text.split()\n",
    "    noise_free_word=[w for w in words if w not in noise_list]\n",
    "    print(noise_free_word)\n",
    "    \n",
    "Remove_noise('this is a sample text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removal of regular expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from analytics vidhya'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample code to remove a regex pattern \n",
    "import re\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"\n",
    "_remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "lem.lemmatize(word, \"v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'god help those who help themselves'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {\"gd\":\"god\", \"thms\":\"themselves\"}\n",
    "def lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "\n",
    "lookup_words(\"gd help those who help themselves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART OF SPEECH TAGGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN'), ('am', 'VBP'), ('vishal', 'JJ'), ('thadari', 'NN'), ('from', 'IN'), ('uttar', 'JJ'), ('pradesh', 'JJ'), ('technical', 'JJ'), ('university', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "text=\"i am vishal thadari from uttar pradesh technical university\"\n",
    "tokens=word_tokenize(text)\n",
    "\n",
    "print(pos_tag(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 2)],\n",
       " [(0, 1),\n",
       "  (9, 1),\n",
       "  (11, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(17, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "\n",
    "#Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "dictionary=corpora.Dictionary(doc_clean)\n",
    "\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.ldamodel.LdaModel'>\n"
     ]
    }
   ],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "print(Lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x7f056e85d780>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.053*\"driving\" + 0.053*\"sister\" + 0.053*\"My\" + 0.053*\"my\" + 0.053*\"to\" + 0.053*\"spends\" + 0.053*\"practice.\" + 0.053*\"of\" + 0.053*\"father\" + 0.053*\"a\"'), (1, '0.063*\"to\" + 0.036*\"not\" + 0.036*\"sugar,\" + 0.036*\"likes\" + 0.036*\"have\" + 0.036*\"father.\" + 0.036*\"bad\" + 0.036*\"consume.\" + 0.036*\"Sugar\" + 0.036*\"is\"'), (2, '0.029*\"driving\" + 0.029*\"sister\" + 0.029*\"my\" + 0.029*\"My\" + 0.029*\"to\" + 0.029*\"stress\" + 0.029*\"blood\" + 0.029*\"suggest\" + 0.029*\"Doctors\" + 0.029*\"may\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NGRAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "generate_ngrams('this is a sample text', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term Frequency – Inverse Document Frequency (TF – IDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.2894590555773325\n",
      "  (0, 7)\t0.578918111154665\n",
      "  (0, 8)\t0.578918111154665\n",
      "  (0, 9)\t0.22479077853914262\n",
      "  (0, 10)\t0.22479077853914262\n",
      "  (0, 6)\t0.38060387289942443\n",
      "  (1, 4)\t0.31725909794672347\n",
      "  (1, 7)\t0.31725909794672347\n",
      "  (1, 8)\t0.6345181958934469\n",
      "  (1, 9)\t0.2463799914078587\n",
      "  (1, 10)\t0.2463799914078587\n",
      "  (1, 3)\t0.41715758779859957\n",
      "  (1, 5)\t0.31725909794672347\n",
      "  (2, 9)\t0.2856167584735664\n",
      "  (2, 10)\t0.2856167584735664\n",
      "  (2, 5)\t0.36778357947820167\n",
      "  (2, 2)\t0.4835912093301898\n",
      "  (2, 1)\t0.4835912093301898\n",
      "  (2, 0)\t0.4835912093301898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['Julie loves me more than Linda loves me',\n",
    "'Jane likes me more than Julie loves me',\n",
    "'He likes basketball more than baseball']\n",
    "x = obj.fit_transform(corpus)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORD EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.009257648\n",
      "[-1.1469857e-03 -3.6010500e-03  2.2427249e-03  4.7994214e-03\n",
      "  2.1721839e-03  3.0026538e-05  4.3467418e-03 -1.2357160e-03\n",
      " -5.7366920e-05  3.0453955e-03  4.0127235e-03  3.8842205e-03\n",
      " -2.8535719e-03  4.0865527e-03 -2.8745893e-03 -1.1017350e-03\n",
      "  4.6013119e-03  3.2573799e-03 -1.3436062e-03  4.0589347e-03\n",
      " -4.4129160e-03 -2.4131478e-03  3.8269418e-03  1.2725377e-03\n",
      " -4.5905374e-03 -8.9009857e-04  1.6610158e-04 -4.1074385e-03\n",
      " -4.4307474e-04  3.4602652e-03  1.3267124e-03  1.7490449e-03\n",
      " -8.9229766e-04 -3.1368199e-03 -2.6469722e-03  3.1454102e-03\n",
      "  5.8836851e-04 -2.1526432e-03  6.9868466e-04  3.8865525e-03\n",
      " -3.1545728e-03  4.0979628e-03  6.4381742e-04 -1.7193075e-03\n",
      "  2.1862900e-03  2.3832980e-03  1.2529517e-03 -4.7981250e-04\n",
      " -2.2237247e-04 -2.4350858e-03 -1.3160603e-03  3.4770514e-05\n",
      "  1.8186569e-03  1.9642464e-03  4.2159003e-03 -3.2436494e-03\n",
      "  1.8906085e-03  4.5736847e-03  1.7972837e-03  1.9164784e-03\n",
      " -2.5563689e-03 -2.6733222e-04  4.5306981e-03  2.0713313e-03\n",
      "  1.0643456e-03  4.4435724e-03 -1.3434784e-03 -3.8427173e-03\n",
      " -1.9192531e-03  3.9277789e-03  3.0838067e-04  3.7989474e-03\n",
      " -1.4235500e-03 -3.4066439e-03  1.1088578e-03  9.6821599e-04\n",
      " -2.7922450e-03 -2.2462860e-03 -4.8428136e-03  2.3102858e-04\n",
      " -4.2824452e-03  3.1812424e-03  4.1569085e-03  1.5007845e-03\n",
      "  1.6488802e-03 -4.4700466e-03 -4.8296889e-03 -9.9592879e-05\n",
      "  1.8769001e-03 -1.9244672e-03 -2.5015038e-03  2.6081384e-03\n",
      "  1.2414247e-03  1.9580845e-03 -3.4550589e-03 -9.5934689e-04\n",
      " -3.9160727e-03 -3.4761152e-03 -4.5300892e-04 -1.2905969e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishalthadari/Documents/env/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  import sys\n",
      "/home/vishalthadari/Documents/env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/home/vishalthadari/Documents/env/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'data science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.similarity('data', 'science')) \n",
    "\n",
    "print (model['learning'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEXT CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"kotak\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LEVENSHTIEN DISTANCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COSINE SIMILARITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an article on analytics vidhya' \n",
    "text2 = 'article on analytics vidhya is about natural language processing'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
